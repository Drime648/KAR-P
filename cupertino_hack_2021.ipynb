{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cupertino hack 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SsY_NyHnkGiy",
        "FdNVAtyNmUMW",
        "tmjRrzE5oSIf",
        "p5t9C_E3aKUX"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNa/FMtNJ/W22hA3uEkiET+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drime648/KAR-P/blob/main/cupertino_hack_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RITYz9vcReHn"
      },
      "source": [
        "#Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov76FH6ZO5jB"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMU__kYhO_M8"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDxRpaF4PHeM"
      },
      "source": [
        "df = pd.read_csv(\"/content/labeled_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hud8HIJPKS9"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMuqc3ZcPdgd"
      },
      "source": [
        "df = df.drop([\"Unnamed: 0\", \"count\", 'hate_speech', 'offensive_language', \"neither\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEb9m5GRQfC8"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WshFJXWSRKWN"
      },
      "source": [
        "class_names = [\"hate speech\", \"offensive language\", \"none\"]\n",
        "num_classes = len(class_names)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJm9V0lPRboM"
      },
      "source": [
        "#split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH_ebyCvRcrp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di31xQ3rRdRN"
      },
      "source": [
        "train_text, val_text, train_labels, val_labels = train_test_split(df[\"tweet\"].to_numpy(),\n",
        "                                                                  df[\"class\"].to_numpy(),\n",
        "                                                                  test_size = 0.1,\n",
        "                                                                  random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJUMfRNNRxkt"
      },
      "source": [
        "len(train_text), len(train_labels), len(val_text), len(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T799KMReSNLt"
      },
      "source": [
        "train_text, train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STr4yCR8STth"
      },
      "source": [
        "#Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QrfaotjSUVd"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnLeA6koSUpt"
      },
      "source": [
        "model_0 = Pipeline([\n",
        "  (\"tfidf\", TfidfVectorizer()),\n",
        "  (\"clf\", MultinomialNB()),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQLE-doNSbLN"
      },
      "source": [
        "model_0.fit(train_text, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm9ld0pJSds9"
      },
      "source": [
        "base_score = model_0.score(val_text, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTAIzHQjSgo8"
      },
      "source": [
        "base_score * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsY_NyHnkGiy"
      },
      "source": [
        "#Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbGKOgP8kH7c"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmX_jAoCkNeN"
      },
      "source": [
        "text_lengths = [len(sentence.split()) for sentence in train_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCiO7IxkQuN"
      },
      "source": [
        "plt.hist(text_lengths, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SAAizJ0k-Yt"
      },
      "source": [
        "max_tokens = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyVbei0alHcd"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYDBpHpjlL4d"
      },
      "source": [
        "text_vectorizer = TextVectorization(output_sequence_length=max_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_11qb2AOlbwf"
      },
      "source": [
        "text_vectorizer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNoIvK2flTMu"
      },
      "source": [
        "len(text_vectorizer.get_vocabulary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPnYiybTmbQI"
      },
      "source": [
        "len_vocab = len(text_vectorizer.get_vocabulary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdNVAtyNmUMW"
      },
      "source": [
        "#Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuWxykgxmVR8"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqJNEa4imYfd"
      },
      "source": [
        "embedding_layer = Embedding(len_vocab, 128, mask_zero=True, name = \"embedding_layer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvpA--ysmZkN"
      },
      "source": [
        "sample_sentence = text_vectorizer([\"hello there larry, my face sucks.\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-vpxLFn9aN"
      },
      "source": [
        "sample_embed = embedding_layer(sample_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX8nelQaoNG8"
      },
      "source": [
        "sample_embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmjRrzE5oSIf"
      },
      "source": [
        "#Make Token Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwsfhqiVoUT8"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_text, val_labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI2lEW0sozgO"
      },
      "source": [
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6-4sswJpMje"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8JZlFo7qXLL"
      },
      "source": [
        "#Make Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdJTtM2FlId0"
      },
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAfgJWgglCqf"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=3, min_lr=0.000001)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMkPTn9oleXZ"
      },
      "source": [
        "early_stopping = EarlyStopping(patience = 6, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xgyHxqJqZw9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G1RTEkiqbdd"
      },
      "source": [
        "inputs = layers.Input(shape = (1,), dtype = tf.string, name = \"inputs\")\n",
        "text_vectors = text_vectorizer(inputs)\n",
        "embeds = embedding_layer(text_vectors)\n",
        "\n",
        "x = layers.Conv1D(128, 3, padding = \"same\", activation=\"relu\")(embeds)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\", name = \"outputs\")(x)\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs, name = \"Model_1\")\n",
        "\n",
        "model_1.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "                optimizer = \"Adam\",\n",
        "                metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf_iqIzsqs5d"
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNg-fphLxmLK"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABkW_rIGrH3O"
      },
      "source": [
        "history_1 = model_1.fit(train_dataset, epochs = 30, steps_per_epoch=len(train_dataset),\n",
        "                        validation_data=val_dataset,\n",
        "                        validation_steps= len(val_dataset),\n",
        "                        callbacks = [early_stopping, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM4mDUa10Ieo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_curves(history):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzYWn8o00f3X"
      },
      "source": [
        "plot_loss_curves(history_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBYHQdSrL_v"
      },
      "source": [
        "model_1.evaluate(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbhWOZIONkKF"
      },
      "source": [
        "tf.keras.models.save_model(model_1, \"hack_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH1fwOC-rc3-"
      },
      "source": [
        "model_1_pred_probs = model_1.predict([\"black people are stupid\"])\n",
        "model_1_pred_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24xQx7_2rkMx"
      },
      "source": [
        "model_1_pred = class_names[np.argmax(model_1_pred_probs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFeKuFzArtMt"
      },
      "source": [
        "model_1_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HIIW0-7rCxR"
      },
      "source": [
        "import numpy as np\n",
        "np.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVMytmmMn-9H"
      },
      "source": [
        "model_1.save('CNN_model')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2FmB19lqgoa"
      },
      "source": [
        "new_cnn = tf.keras.models.load_model(\"/content/CNN_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0e445V0qsmZ"
      },
      "source": [
        "new_cnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ_M2q_8XWQ-"
      },
      "source": [
        "#Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA4zhZTCXXSQ"
      },
      "source": [
        "def split_chars(text):\n",
        "  return \" \".join(list(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox5v1GfuYDrQ"
      },
      "source": [
        "train_chars = [split_chars(line) for line in train_text]\n",
        "val_chars = [split_chars(line) for line in val_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAWQGhPgYFcO"
      },
      "source": [
        "train_chars[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Q0f94RYGLO"
      },
      "source": [
        "char_lengths = [len(char) for char in train_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC9aTfGAYKOO"
      },
      "source": [
        "cover_most_chars = int(np.percentile(char_lengths, 95))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIJ6-jo3YLU-"
      },
      "source": [
        "cover_most_chars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm4dHBfvYMv-"
      },
      "source": [
        "num_char_tokens = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY7PsKIkYQke"
      },
      "source": [
        "char_vectorizer = TextVectorization(num_char_tokens, output_sequence_length=cover_most_chars, name = \"char_vectorizing_layer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Rmw9w9YS7d"
      },
      "source": [
        "char_vectorizer.adapt(train_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMQ9Zf6zYdyP"
      },
      "source": [
        "char_len_vocab = len(char_vectorizer.get_vocabulary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCLS1o6dYlf_"
      },
      "source": [
        "char_embedding_layer = Embedding(char_len_vocab, 25, mask_zero=True, name = \"char_embedding_layer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK4jpQ6wYouu"
      },
      "source": [
        "char_train_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels))\n",
        "char_val_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels))\n",
        "\n",
        "\n",
        "char_train_dataset = char_train_dataset.batch(32)\n",
        "char_train_dataset = char_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "char_val_dataset = char_val_dataset.batch(32)\n",
        "char_val_dataset = char_val_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "754m-7S0ZEC_"
      },
      "source": [
        "char_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFSDmIb7ZFOu"
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFuqc9c6ZHXO"
      },
      "source": [
        "embed_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=False, name = \"encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxlcEhcUZLie"
      },
      "source": [
        "embeddings = embed_layer([\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I am a sentence for which I would like to get its embedding\"])\n",
        "\n",
        "print(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l22VMMpaZNQv"
      },
      "source": [
        "token_inputs = layers.Input(shape = (1,), dtype = tf.string, name = \"token_inputs\")\n",
        "text_vectors = text_vectorizer(token_inputs)\n",
        "embeds = embedding_layer(text_vectors)\n",
        "x = layers.Conv1D(128, 3, padding = \"same\", activation=\"relu\")(embeds)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "token_model = tf.keras.Model(inputs = token_inputs, outputs = x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4U8WQhUZP_P"
      },
      "source": [
        "char_inputs = layers.Input(shape = (1,), dtype = tf.string, name = \"char_inputs\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embedding_layer(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
        "char_model = tf.keras.Model(char_inputs, char_bi_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m28HLX5HZRO_"
      },
      "source": [
        "combined_embeddings = layers.Concatenate(name = \"concat\")([token_model.output, char_model.output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzwtxiFoZUz_"
      },
      "source": [
        "hybrid_dropout = layers.Dropout(0.5)(combined_embeddings)\n",
        "hybrid_dense = layers.Dense(128, activation = \"relu\")(hybrid_dropout)\n",
        "end_dropout = layers.Dropout(0.5)(hybrid_dense)\n",
        "output = layers.Dense(num_classes, activation = \"softmax\")(end_dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J78u1ENaZd7P"
      },
      "source": [
        "model_2 = tf.keras.Model(inputs = [token_model.input, char_model.input],\n",
        "                         outputs = output,\n",
        "                         name = \"Model_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzhgXBLlZsqP"
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_yaPRDyZt5e"
      },
      "source": [
        "model_2.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "                optimizer = \"Adam\",\n",
        "                metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsMXzd3eaACN"
      },
      "source": [
        "hybrid_train_data = tf.data.Dataset.from_tensor_slices((train_text, train_chars))\n",
        "hybrid_train_labels = tf.data.Dataset.from_tensor_slices(train_labels)\n",
        "hybrid_train_dataset = tf.data.Dataset.zip((hybrid_train_data, hybrid_train_labels))\n",
        "\n",
        "hybrid_train_dataset = hybrid_train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "hybrid_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGTuJ7zvaCGq"
      },
      "source": [
        "hybrid_val_data = tf.data.Dataset.from_tensor_slices((val_text, val_chars))\n",
        "hybrid_val_labels = tf.data.Dataset.from_tensor_slices(val_labels)\n",
        "hybrid_val_dataset = tf.data.Dataset.zip((hybrid_val_data, hybrid_val_labels))\n",
        "\n",
        "hybrid_val_dataset = hybrid_val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "hybrid_val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF7zuNHaZySO"
      },
      "source": [
        "history_2 = model_2.fit(hybrid_train_dataset, epochs = 30, steps_per_epoch=len(hybrid_train_dataset),\n",
        "                        validation_data=hybrid_val_dataset,\n",
        "                        validation_steps= int(0.1 * len(hybrid_val_dataset)),\n",
        "                        callbacks = [early_stopping, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYbrK60vp09J"
      },
      "source": [
        "model_2.evaluate(hybrid_val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKKpn6-BecIx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn_T0CWIy424"
      },
      "source": [
        "#Failed experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n63MMH9Sy6TU"
      },
      "source": [
        "inputs = layers.Input(shape = (1,), dtype = tf.string, name = \"inputs\")\n",
        "text_vectors = text_vectorizer(inputs)\n",
        "embeds = embedding_layer(text_vectors)\n",
        "\n",
        "x = layers.LSTM(units = 64, return_sequences=True)(x) # inputs = 3 dimensions, output = 3 dimensions. Return sequences must be true when stackng RNN\n",
        "x = layers.LSTM(64)(x) # inputs = 3 dimensions, output = 2 dimensions\n",
        "\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\", name = \"outputs\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs, name = \"Model_1\")\n",
        "\n",
        "model_2.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "                optimizer = \"Adam\",\n",
        "                metrics = [\"accuracy\"])\n",
        "\n",
        "history_2 = model_2.fit(train_dataset, epochs = 30, steps_per_epoch=len(train_dataset),\n",
        "                        validation_data=val_dataset,\n",
        "                        validation_steps= len(val_dataset),\n",
        "                        callbacks = [early_stopping, reduce_lr])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxsU1bS_zbie"
      },
      "source": [
        "inputs = layers.Input(shape = (1,), dtype = tf.string, name = \"inputs\")\n",
        "text_vectors = text_vectorizer(inputs)\n",
        "embeds = embedding_layer(text_vectors)\n",
        "\n",
        "x = layers.GRU(units = 64, return_sequences=True)(x) # inputs = 3 dimensions, output = 3 dimensions. Return sequences must be true when stackng RNN\n",
        "x = layers.GRU(64)(x) # inputs = 3 dimensions, output = 2 dimensions\n",
        "\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\", name = \"outputs\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs, name = \"Model_1\")\n",
        "\n",
        "model_2.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "                optimizer = \"Adam\",\n",
        "                metrics = [\"accuracy\"])\n",
        "\n",
        "history_2 = model_2.fit(train_dataset, epochs = 30, steps_per_epoch=len(train_dataset),\n",
        "                        validation_data=val_dataset,\n",
        "                        validation_steps= len(val_dataset),\n",
        "                        callbacks = [early_stopping, reduce_lr])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5t9C_E3aKUX"
      },
      "source": [
        "#predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLh8Ww7saLQD"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/lexicons/refined_ngram_dict.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7idv2rfnaMuk"
      },
      "source": [
        "import pandas as pd\n",
        "targets = pd.read_csv(\"/content/refined_ngram_dict.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTJy74-WaaPz"
      },
      "source": [
        "targets = targets.drop(\"prophate\", axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqGQxWGUbCQj"
      },
      "source": [
        "targets = targets[\"ngram\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGv0N1pdhAmK"
      },
      "source": [
        "import pandas as pd\n",
        "pd.read_csv(\"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/lexicons/refined_ngram_dict.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjBjZ98BbGej"
      },
      "source": [
        "\"allah akbar\" == targets[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}